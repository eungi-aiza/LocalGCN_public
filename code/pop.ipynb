{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# make sure to change args into args = parser.parse_args(args=[]) in parse.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import world\n",
    "import utils\n",
    "import torch\n",
    "import os\n",
    "import dataloader\n",
    "import model\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    models = {\n",
    "        \"localgcn\": model.LocalGCN,\n",
    "        \"lgn\" : model.LightGCN\n",
    "    }\n",
    "    return models[model_name]\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    return dataloader.Loader(path=dataset_path)\n",
    "\n",
    "def test_one_batch(X):\n",
    "    sorted_items = X[0].numpy()\n",
    "    gt = X[1]\n",
    "    r = utils.getLabel(gt, sorted_items)\n",
    "    pre, recall, ndcg = [], [], []\n",
    "    for k in world.config[\"topks\"]:\n",
    "        ret = utils.recall_precision_at_K(gt, r, k)\n",
    "        pre.append(ret[\"precision\"])\n",
    "        recall.append(ret[\"recall\"])\n",
    "        ndcg.append(utils.NDCG_at_K(gt, r, k))\n",
    "    return {\"recall\": np.array(recall), \"precision\": np.array(pre), \"ndcg\": np.array(ndcg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 15:24:57,478 INFO  [dataloader.py:17] loading [/home/s1/eungikim/Research/LocalGCN_public/data/ml-1m]\n",
      "2024-01-15 15:24:57,708 INFO  [dataloader.py:47] 407184 interactions for training\n",
      "2024-01-15 15:24:57,710 INFO  [dataloader.py:48] 112428 interactions for testing\n",
      "2024-01-15 15:24:57,711 INFO  [dataloader.py:50] ml-1m Sparsity : 0.027547671321246144\n",
      "2024-01-15 15:24:57,711 INFO  [dataloader.py:52] number of users : 6034\n",
      "2024-01-15 15:24:57,712 INFO  [dataloader.py:53] number of items : 3126\n",
      "2024-01-15 15:24:58,411 INFO  [dataloader.py:68] ml-1m is ready to go\n"
     ]
    }
   ],
   "source": [
    "# GET Dataset\n",
    "dataname = \"ml-1m\"\n",
    "world.config['dataset'] = dataname\n",
    "data_path = os.path.join(world.DATA_PATH, dataname)\n",
    "dataset = load_dataset(dataset_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3125it [00:00, 231723.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# GET POP\n",
    "item, count = np.unique(dataset.train_item, return_counts=True)\n",
    "pop = torch.zeros(dataset.m_items)\n",
    "for i, c in tqdm(zip(item, count)):\n",
    "    pop[i] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 15:24:58,530 INFO  [<ipython-input-31-aba5d156a18f>:16] test_u_batch_size is too big for this dataset, try a small one 603\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL BATCH: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00,  7.78it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.80it/s]\n"
     ]
    }
   ],
   "source": [
    "u_batch_size = world.config[\"test_u_batch_size\"]\n",
    "test_dict: dict = dataset.test_dict\n",
    "\n",
    "# eval mode with no dropout\n",
    "max_K = max(world.config[\"topks\"])\n",
    "results = {\n",
    "    \"precision\": np.zeros(len(world.config[\"topks\"])),\n",
    "    \"recall\": np.zeros(len(world.config[\"topks\"])),\n",
    "    \"ndcg\": np.zeros(len(world.config[\"topks\"])),\n",
    "}\n",
    "\n",
    "users = list(test_dict.keys())\n",
    "try:\n",
    "    assert u_batch_size <= len(users) / 10\n",
    "except AssertionError:\n",
    "    world.LOGGER.info(f\"test_u_batch_size is too big for this dataset, try a small one {len(users) // 10}\")\n",
    "users_list = []\n",
    "rating_list = []\n",
    "gt_list = []\n",
    "total_batch = len(users) // u_batch_size + 1\n",
    "test_loader = tqdm(utils.minibatch(users, batch_size=u_batch_size))\n",
    "\n",
    "print(\"TOTAL BATCH:\", total_batch)\n",
    "for batch_users in tqdm(test_loader, total=total_batch):\n",
    "    all_pos = dataset.get_user_pos_items(batch_users)\n",
    "    gt = [test_dict[u] for u in batch_users]\n",
    "    batch_users = torch.Tensor(batch_users).long()\n",
    "\n",
    "    rating = pop.unsqueeze(0).repeat(batch_users.shape[0], 1)\n",
    "    exclude_index = []\n",
    "    exclude_items = []\n",
    "    for range_i, items in enumerate(all_pos):\n",
    "        exclude_index.extend([range_i] * len(items))\n",
    "        exclude_items.extend(items)\n",
    "    rating[exclude_index, exclude_items] = -(1 << 10)\n",
    "    _, rating_K = torch.topk(rating, k=max_K)\n",
    "    rating = rating.cpu().numpy()\n",
    "    del rating\n",
    "    users_list.append(batch_users)\n",
    "    rating_list.append(rating_K.cpu())\n",
    "    gt_list.append(gt)\n",
    "assert total_batch == len(users_list)\n",
    "\n",
    "X = zip(rating_list, gt_list)\n",
    "pre_results = []\n",
    "for x in X:\n",
    "    pre_results.append(test_one_batch(x))\n",
    "scale = float(u_batch_size / len(users))\n",
    "for result in pre_results:\n",
    "    results[\"recall\"] += result[\"recall\"]\n",
    "    results[\"precision\"] += result[\"precision\"]\n",
    "    results[\"ndcg\"] += result[\"ndcg\"]\n",
    "results[\"recall\"] /= float(len(users))\n",
    "results[\"precision\"] /= float(len(users))\n",
    "results[\"ndcg\"] /= float(len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 15:24:59,578 INFO  [<ipython-input-32-9ee1c07100a8>:1] {'precision': array([0.15545244, 0.13215114, 0.11823003, 0.10077892]), 'recall': array([0.01262393, 0.0458194 , 0.08138716, 0.13433756]), 'ndcg': array([0.15545244, 0.14241747, 0.1397087 , 0.14522995])}\n"
     ]
    }
   ],
   "source": [
    "world.LOGGER.info(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 21:11:30,713 INFO  [<ipython-input-11-3b42ed2ae836>:1] model>> POP\n"
     ]
    }
   ],
   "source": [
    "world.LOGGER.info(\"model>> POP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kwangeun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
